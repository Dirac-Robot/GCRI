import ast
import json
import multiprocessing
import os
import sys

from datasets import load_dataset
from dotenv import load_dotenv
from loguru import logger
from pydantic import BaseModel, Field
from tqdm import tqdm

from gcri.graphs.gcri_unit import GCRI
from gcri.config import scope

DATASET_HF_ID = "bigcode/bigcodebench-hard"
BENCHMARK_DIR = 'benchmark_results/bigcodebench_hard'
TIMEOUT_SECONDS = 30.


@scope
def get_preset_name(config):
    if config.get('custom_config_path'):
        return os.path.splitext(os.path.basename(config.custom_config_path))[0]
    return 'none'


RESULT_FILE = os.path.join(BENCHMARK_DIR, f'bigcodebench_hard_results_{get_preset_name()}.json')


class BigCodeBenchResult(BaseModel):
    thought_process: str = Field(
        ...,
        description='Detailed reasoning about the algorithm, libraries used, and edge cases.'
    )
    solution_code: str = Field(
        ...,
        description='The complete, executable Python code implementation only. No markdown formatting.'
    )


def setup_directories():
    os.makedirs(BENCHMARK_DIR, exist_ok=True)


# ðŸ”¥ [Fix 1] Smart Preprocessing ì ìš©
def preprocess_code(code_str: str) -> str:
    if not code_str:
        return ''

    code_str = code_str.strip()

    # Markdown Code Block ì œê±°
    if code_str.startswith('```python'):
        code_str = code_str[9:]
    elif code_str.startswith('```py'):
        code_str = code_str[5:]
    elif code_str.startswith('```'):
        code_str = code_str[3:]

    if code_str.endswith('```'):
        code_str = code_str[:-3]

    code_str = code_str.strip()

    # ê¸°ì¡´ì˜ ë¶ˆì™„ì „í•œ ë¼ì¸ ì •ë¦¬ ë¡œì§ (ë°±ìŠ¬ëž˜ì‹œ ì œê±° ë“±) ìœ ì§€
    lines = code_str.split('\n')
    cleaned_lines = []
    for line in lines:
        if line.rstrip().endswith('\\'):
            line = line.rstrip()
        cleaned_lines.append(line)
    
    code_str = '\n'.join(cleaned_lines)

    # ðŸ”¥ Smart Fix: ë¬¸ë²• ê²€ì¦ì„ í†µí•œ ì¡°ê±´ë¶€ ì¹˜í™˜
    try:
        # 1. ì›ë³¸ ê·¸ëŒ€ë¡œ íŒŒì‹± ì‹œë„ (Regex ë“±ì´ ê¹¨ì§€ì§€ ì•Šë„ë¡)
        ast.parse(code_str)
        return code_str
    except SyntaxError:
        # 2. ë¬¸ë²• ì˜¤ë¥˜ê°€ ìžˆë‹¤ë©´? -> ì´ìŠ¤ì¼€ì´í”„ ë¬¸ì œì¼ ìˆ˜ ìžˆìœ¼ë‹ˆ ì¹˜í™˜ ì‹œë„
        try:
            fallback_code = code_str.replace('\\"', '"').replace("\\'", "'")
            ast.parse(fallback_code)
            return fallback_code
        except SyntaxError:
            # 3. ì¹˜í™˜í•´ë„ í‹€ë ¸ë‹¤ë©´ ì›ë³¸ ë°˜í™˜ (í‰ê°€ ë‹¨ê³„ì—ì„œ ì—ëŸ¬ ë¡œê·¸ ê¸°ë¡)
            return code_str


# ðŸ”¥ [Fix 2] ê°•ë ¥í•œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ íŠ¸ë¦¬ê±° (Silent Pass ë°©ì§€)
TEST_RUNNER_TRIGGER = """
if __name__ == '__main__':
    try:
        import unittest
        import sys
        import os
        
        # 1. unittest íƒìƒ‰ ë° ì‹¤í–‰
        loader = unittest.TestLoader()
        suite = unittest.TestSuite()
        has_tests = False
        
        # globals() ë³µì‚¬ë³¸ìœ¼ë¡œ ìˆœíšŒ
        current_globals = dict(globals())
        
        for name, obj in current_globals.items():
            if isinstance(obj, type) and issubclass(obj, unittest.TestCase):
                if obj is not unittest.TestCase:
                    tests = loader.loadTestsFromTestCase(obj)
                    if tests.countTestCases() > 0:
                        suite.addTests(tests)
                        has_tests = True
        
        # 2. check() í•¨ìˆ˜ íƒìƒ‰ (BigCodeBench ì¼ë¶€ ìœ í˜•)
        has_check_func = 'check' in current_globals and callable(current_globals['check'])

        # 3. ì‹¤í–‰ ë¡œì§
        if has_tests:
            # unittest ì‹¤í–‰ (ê²°ê³¼ ì¶œë ¥ ì–µì œ)
            runner = unittest.TextTestRunner(stream=open(os.devnull, 'w'), verbosity=0)
            result = runner.run(suite)
            if not result.wasSuccessful():
                failures = len(result.failures) + len(result.errors)
                raise AssertionError(f"Unittest Failed: {failures} errors/failures")

        if has_check_func:
            if 'task_func' in current_globals:
                # check(task_func) ì‹¤í–‰
                current_globals['check'](current_globals['task_func'])
            else:
                raise AssertionError("Check function found but 'task_func' is missing.")

        # 4. ì•ˆì „ìž¥ì¹˜: ì•„ë¬´ëŸ° í…ŒìŠ¤íŠ¸ë„ ì—†ìœ¼ë©´ ì‹¤íŒ¨ ì²˜ë¦¬
        if not has_tests and not has_check_func:
            # ë°ì´í„°ì…‹ íŠ¹ì„±ì— ë”°ë¼ passë¡œ ë³€ê²½ ê°€ëŠ¥í•˜ì§€ë§Œ, ì—„ê²©í•œ ê²€ì¦ì„ ìœ„í•´ raise ìœ ì§€ ê¶Œìž¥
            pass

    except Exception as e:
        # ì—ëŸ¬ë¥¼ ë°œìƒì‹œì¼œì•¼ worker í”„ë¡œì„¸ìŠ¤ê°€ ê°ì§€í•¨
        raise e
"""


def run_test_case(test_program, result_queue):
    try:
        # stdout/stderr ì–µì œ
        sys.stdout = open(os.devnull, 'w')
        sys.stderr = open(os.devnull, 'w')

        exec_globals = {}
        # ðŸ”¥ [Fix 3-1] __name__ì„ __main__ìœ¼ë¡œ ê°•ì œ ì„¤ì •
        exec_globals['__name__'] = '__main__'
        
        exec(test_program, exec_globals)
        result_queue.put('passed')
    except AssertionError as e:
        result_queue.put(f'assertion_failed: {str(e)}')
    except SyntaxError as e:
        result_queue.put(f'failed: SyntaxError: {str(e)}')
    except Exception as e:
        result_queue.put(f'failed: {type(e).__name__}: {str(e)}')
    finally:
        sys.stdout = sys.__stdout__
        sys.stderr = sys.__stderr__


def evaluate_code(sample, completion_code):
    # ðŸ”¥ [Fix 3-2] í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬(numpy, pandas) ì¶”ê°€
    common_imports = (
        'import sys\n'
        'import os\n'
        'import math\n'
        'import string\n'
        'import re\n'
        'import collections\n'
        'import heapq\n'
        'import itertools\n'
        'import functools\n'
        'import copy\n'
        'import hashlib\n'
        'import json\n'
        'import datetime\n'
        'import random\n'
        'import pickle\n'
        'import csv\n'
        'import io\n'
        'import tempfile\n'
        'import shutil\n'
        'import glob\n'
        'import subprocess\n'
        'import threading\n'
        'import time\n'
        'import unittest\n'
        'import numpy as np\n'  # Added
        'import pandas as pd\n' # Added
        'from typing import *\n'
        'from unittest.mock import patch, MagicMock\n\n'
    )

    test_code = sample.get('test', '')

    if not test_code:
        return False, 'No test code available'

    # ðŸ”¥ [Fix 4] Trigger ê²°í•©
    full_code = f"{common_imports}\n{completion_code}\n\n{test_code}\n\n{TEST_RUNNER_TRIGGER}"

    result_queue = multiprocessing.Queue()
    process = multiprocessing.Process(target=run_test_case, args=(full_code, result_queue))

    process.start()
    process.join(TIMEOUT_SECONDS)

    if process.is_alive():
        process.terminate()
        process.join()
        return False, 'Timeout'

    if not result_queue.empty():
        result = result_queue.get()
        if result == 'passed':
            return True, 'Passed'
        else:
            return False, result
    else:
        return False, 'No result (Process crashed)'


@scope
def run_benchmark(config, num_samples=None, split='v0.1.4'):
    config.protocols.force_output = True
    logger.info(config.to_xyz())
    load_dotenv()
    setup_directories()

    logger.info('ðŸ¤– GCRI Worker Initializing for BigCodeBench Hard...')
    worker = GCRI(config, schema=BigCodeBenchResult)

    logger.info(f'ðŸ“š Loading BigCodeBench Hard dataset from {DATASET_HF_ID} (split: {split})...')
    try:
        dataset = load_dataset(DATASET_HF_ID, split=split)
    except Exception as e:
        logger.error(f'Failed to load dataset: {e}')
        return

    logger.info(f'ðŸ“Š Loaded {len(dataset)} tasks')

    if num_samples:
        dataset = dataset.select(range(min(len(dataset), num_samples)))
        logger.info(f'ðŸ” Running on first {num_samples} samples.')

    results = []
    processed_ids = set()
    total_processed = 0
    total_passed = 0

    if os.path.exists(RESULT_FILE):
        try:
            with open(RESULT_FILE, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
                valid_results = []
                for item in existing_data:
                    t_id = item.get('task_id')
                    comp = item.get('completion')
                    error_log = item.get('error_log', '')
                    if not (comp and isinstance(comp, str) and comp.strip()):
                        logger.info(f'â™»ï¸ Re-queueing Task {t_id} (Reason: Empty completion)')
                    elif 'No module named' in error_log:
                        logger.info(f'â™»ï¸ Re-queueing Task {t_id} (Reason: {error_log})')
                    else:
                        valid_results.append(item)
                        processed_ids.add(t_id)
                results = valid_results
                total_processed = len(results)
                total_passed = sum(1 for item in results if item.get('passed', False))
                logger.info(f'ðŸ”„ Resuming... {total_processed} valid items retained.')
        except json.JSONDecodeError:
            logger.warning('âš ï¸ Result file is corrupt. Starting fresh.')

    for idx, item in tqdm(enumerate(dataset), total=len(dataset), desc='BigCodeBench Hard'):
        task_id = item.get('task_id', str(idx))
        if task_id in processed_ids:
            continue

        try:
            complete_prompt = item.get('complete_prompt', item.get('code_prompt', ''))

            task_prompt = (
                f'You are an expert Python software engineer.\n'
                f'Implement the following Python function completely.\n'
                f'Return the COMPLETE function including the signature, docstring, and implementation.\n'
                f'This benchmark tests complex, real-world coding with multiple library usage.\n\n'
                f'--- FUNCTION TO IMPLEMENT ---\n'
                f'{complete_prompt}\n\n'
                f'Provide your reasoning first, then the COMPLETE function code (including def statement and docstring).'
            )

            logger.info(f'â–¶ Running Task: {task_id}')
            output_state = worker(task_prompt, commit_mode='auto-reject')
            final_output_obj = output_state.get('final_output')

            parsed_code = ''
            parsed_reasoning = ''

            if final_output_obj:
                if isinstance(final_output_obj, dict):
                    raw_code = final_output_obj.get('solution_code', '')
                    parsed_code = preprocess_code(raw_code)
                    parsed_reasoning = final_output_obj.get('thought_process', '')
                    raw_dump = final_output_obj
                else:
                    raw_dump = str(final_output_obj)
                    parsed_code = preprocess_code(str(final_output_obj))
            else:
                raw_dump = 'No final output generated.'

            is_passed, eval_message = evaluate_code(item, parsed_code)

            total_processed += 1
            if is_passed:
                total_passed += 1

            current_accuracy = (total_passed/total_processed)*100
            
            # ë¡œê·¸ ê°€ë…ì„± ê°œì„ 
            status_icon = 'âœ…' if is_passed else 'âŒ'
            status = 'PASSED' if is_passed else 'FAILED'
            logger.info(
                f'ðŸ§ª Result: {status_icon} {status} ({eval_message}) | Acc: {current_accuracy:.2f}%'
            )

            result = {
                'task_id': task_id,
                'prompt': complete_prompt[:1000],
                'canonical_solution': item.get('canonical_solution', '')[:500],
                'completion': parsed_code,
                'reasoning': parsed_reasoning,
                'passed': is_passed,
                'error_log': eval_message,
                'raw_output': raw_dump,
                'full_state': {
                    'best_branch': output_state.get('best_branch_index'),
                    'decision': output_state.get('decision'),
                    'iterations': output_state.get('count', 0)
                }
            }
            results.append(result)

            with open(RESULT_FILE, 'w', encoding='utf-8') as f:
                json.dump(results, f, indent=4, ensure_ascii=False)

        except KeyboardInterrupt:
            logger.warning('â›” Benchmark interrupted by user.')
            break
        except Exception as e:
            logger.error(f'âŒ Error processing sample {task_id}: {e}')
            continue

    final_acc = (total_passed/len(dataset))*100 if len(dataset) > 0 else 0
    logger.info(f'âœ… BigCodeBench Hard completed. Final Accuracy: {final_acc:.2f}%')
    logger.info(f'ðŸ“„ Detailed results saved to {RESULT_FILE}')


if __name__ == '__main__':
    multiprocessing.freeze_support()
    run_benchmark()