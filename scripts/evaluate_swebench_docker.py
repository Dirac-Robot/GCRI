#!/usr/bin/env python3
"""
SWE-bench Docker Evaluation Script

This script runs the official SWE-bench evaluation using Docker.
It takes the predictions JSONL file generated by benchmark_swebench_lite.py
and evaluates them using the SWE-bench harness.

Prerequisites:
  - Docker installed and running
  - At least 120GB free disk space
  - 16GB+ RAM recommended
  - pip install swebench

Usage:
  python scripts/evaluate_swebench_docker.py --predictions benchmark_results/swebench_lite/predictions_xxx.jsonl

For macOS Docker Desktop, ensure resources are set to:
  - CPUs: 8+
  - Memory: 16GB+
  - Disk: 120GB+
"""

import argparse
import json
import os
import subprocess
import sys
from datetime import datetime
from pathlib import Path

from loguru import logger


BENCHMARK_DIR = 'benchmark_results/swebench_lite'
DATASET_NAME = 'princeton-nlp/SWE-bench_Lite'
SPLIT = 'test'


def check_docker():
    """Verify Docker is installed and running."""
    try:
        result = subprocess.run(
            ['docker', 'info'],
            capture_output=True,
            text=True,
            timeout=30
        )
        if result.returncode != 0:
            logger.error('Docker is not running. Please start Docker Desktop.')
            return False
        logger.info('‚úÖ Docker is running.')
        return True
    except FileNotFoundError:
        logger.error('Docker is not installed. Please install Docker first.')
        return False
    except subprocess.TimeoutExpired:
        logger.error('Docker check timed out.')
        return False


def check_swebench_installed():
    """Check if swebench package is installed."""
    try:
        import swebench
        logger.info(f'‚úÖ SWE-bench package found: {swebench.__version__ if hasattr(swebench, "__version__") else "installed"}')
        return True
    except ImportError:
        logger.warning('‚ö†Ô∏è SWE-bench package not found. Installing...')
        try:
            subprocess.run(
                [sys.executable, '-m', 'pip', 'install', 'swebench', '-q'],
                check=True
            )
            logger.info('‚úÖ SWE-bench installed successfully.')
            return True
        except subprocess.CalledProcessError as e:
            logger.error(f'Failed to install swebench: {e}')
            return False


def validate_predictions_file(predictions_path: str) -> int:
    """Validate the predictions JSONL file and return count."""
    if not os.path.exists(predictions_path):
        logger.error(f'Predictions file not found: {predictions_path}')
        return 0

    count = 0
    with open(predictions_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            try:
                pred = json.loads(line.strip())
                if 'instance_id' not in pred or 'model_patch' not in pred:
                    logger.warning(f'Line {line_num}: Missing required fields.')
                    continue
                count += 1
            except json.JSONDecodeError as e:
                logger.warning(f'Line {line_num}: Invalid JSON - {e}')

    logger.info(f'üìä Found {count} valid predictions in file.')
    return count


def run_evaluation(predictions_path: str, max_workers: int = 4, run_id: str = None):
    """Run the SWE-bench Docker evaluation."""
    if run_id is None:
        run_id = datetime.now().strftime('%Y%m%d_%H%M%S')

    output_dir = os.path.join(BENCHMARK_DIR, f'eval_results_{run_id}')
    os.makedirs(output_dir, exist_ok=True)

    logger.info(f'üöÄ Starting SWE-bench evaluation...')
    logger.info(f'   Predictions: {predictions_path}')
    logger.info(f'   Output dir:  {output_dir}')
    logger.info(f'   Workers:     {max_workers}')

    cmd = [
        sys.executable, '-m', 'swebench.harness.run_evaluation',
        '--dataset_name', DATASET_NAME,
        '--split', SPLIT,
        '--predictions_path', predictions_path,
        '--max_workers', str(max_workers),
        '--run_id', run_id,
    ]

    logger.info(f'Running: {" ".join(cmd)}')

    try:
        process = subprocess.Popen(
            cmd,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            bufsize=1
        )

        for line in iter(process.stdout.readline, ''):
            print(line, end='')
            if 'resolved' in line.lower() or 'passed' in line.lower():
                logger.info(line.strip())

        process.wait()

        if process.returncode == 0:
            logger.info(f'‚úÖ Evaluation completed successfully!')
            logger.info(f'üìÅ Results saved to: {output_dir}')
        else:
            logger.error(f'‚ùå Evaluation failed with return code: {process.returncode}')

        return process.returncode

    except KeyboardInterrupt:
        logger.warning('‚õî Evaluation interrupted by user.')
        process.terminate()
        return 1
    except Exception as e:
        logger.error(f'Evaluation error: {e}')
        return 1


def generate_report(run_id: str):
    """Generate a summary report from evaluation results."""
    results_dir = os.path.join(BENCHMARK_DIR, f'eval_results_{run_id}')

    report_file = None
    for f in Path(results_dir).glob('*.json'):
        if 'results' in f.name.lower():
            report_file = f
            break

    if report_file and report_file.exists():
        with open(report_file, 'r') as f:
            results = json.load(f)

        resolved = sum(1 for r in results.values() if r.get('resolved', False))
        total = len(results)

        logger.info('\n' + '='*50)
        logger.info('üìä EVALUATION SUMMARY')
        logger.info('='*50)
        logger.info(f'Total instances:   {total}')
        logger.info(f'Resolved:          {resolved}')
        logger.info(f'Success rate:      {resolved/total*100:.2f}%' if total > 0 else 'N/A')
        logger.info('='*50)

        summary_path = os.path.join(results_dir, 'summary.txt')
        with open(summary_path, 'w') as f:
            f.write(f'SWE-bench Lite Evaluation Summary\n')
            f.write(f'Run ID: {run_id}\n')
            f.write(f'Total: {total}\n')
            f.write(f'Resolved: {resolved}\n')
            f.write(f'Success Rate: {resolved/total*100:.2f}%\n' if total > 0 else 'N/A\n')

        logger.info(f'Summary saved to: {summary_path}')


def main():
    parser = argparse.ArgumentParser(
        description='Run SWE-bench Docker evaluation on generated patches.',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog=__doc__
    )
    parser.add_argument(
        '--predictions', '-p',
        type=str,
        required=True,
        help='Path to predictions JSONL file'
    )
    parser.add_argument(
        '--max-workers', '-w',
        type=int,
        default=4,
        help='Number of parallel workers (default: 4)'
    )
    parser.add_argument(
        '--run-id',
        type=str,
        default=None,
        help='Custom run ID (default: timestamp)'
    )
    parser.add_argument(
        '--skip-docker-check',
        action='store_true',
        help='Skip Docker availability check'
    )

    args = parser.parse_args()

    logger.info('üê≥ SWE-bench Docker Evaluation')
    logger.info('='*50)

    if not args.skip_docker_check:
        if not check_docker():
            sys.exit(1)

    if not check_swebench_installed():
        sys.exit(1)

    pred_count = validate_predictions_file(args.predictions)
    if pred_count == 0:
        logger.error('No valid predictions found. Exiting.')
        sys.exit(1)

    run_id = args.run_id or datetime.now().strftime('%Y%m%d_%H%M%S')

    returncode = run_evaluation(
        predictions_path=args.predictions,
        max_workers=args.max_workers,
        run_id=run_id
    )

    if returncode == 0:
        generate_report(run_id)

    sys.exit(returncode)


if __name__ == '__main__':
    main()
