# AI 에이전트에게 필요한 건 자기반성이 아니라, 반증이다

## self-reflection이 아니라 falsification으로 피드백 루프를 만들어야 하는 이유

---

지금 AI 에이전트를 만드는 사람들은 대부분 self-reflection에 집착하고 있다. 직관적이다. 모델이 자기 출력을 보고, 뭐가 잘못됐는지 파악하고, 다시 시도한다. Reflexion (Shinn et al., 2023), Self-Refine (Madaan et al., 2023) 같은 프레임워크가 전부 이 패턴이다. 모델이 생성하고, 모델이 비평하고, 모델이 수정한다.

문제가 있다. 미묘한 문제가 아니다.

Huang et al.이 ICLR 2024에서 발표한 논문 제목이 "Large Language Models Cannot Self-Correct Reasoning Yet"이다. 결론은 명확하다: LLM이 외부 피드백 없이 자기 추론을 고치려 하면, 성능이 오히려 *떨어지는* 경우가 많다. 실수를 만든 모델이 그 실수를 평가하는 모델과 동일하다. 편향된 판사에게 자기 판결을 재심하라고 하는 것과 같다.

구현 세부사항의 문제가 아니다. LLM 에이전트의 피드백 루프 설계 자체의 구조적 결함이다.

---

## Self-reflection의 함정

Self-reflection 프레임워크들은 구조가 같다. 하나의 모델이 (혹은 같은 모델에 다른 프롬프트를 줘서) 출력을 보고, 자연어로 비평을 쓰고, 다시 피드백한다. Reflexion은 이 반성문을 episodic memory buffer에 저장한다. Self-Refine은 비평-수정 사이클을 여러 번 돌린다. LATS는 tree search를 얹는다. 그런데 이 모든 프레임워크가 평가자에게 하는 질문은 결국 이것이다:

*"이거 맞아? 뭘 개선할 수 있을까?"*

이건 open-ended evaluation이다. 평가자가 정확성을 이해하면서 동시에 문제점을 찾고, 수정까지 제안해야 한다. 사람도 어려운 일이다. LLM한테 시키면 잘 문서화된 실패 모드가 발동한다: sycophancy. 모델은 그럴듯하고, 구조가 깔끔하고, 자신감 있어 보이는 출력을 승인하는 경향이 있다. 틀렸더라도. 깔끔한 로직으로 포장된 hallucination은 self-critique 루프를 통과한다.

Multi-agent debate (Du et al., 2023)은 여러 모델을 서로 토론시켜서 이걸 해결하려 한다. 하지만 debate은 대칭적이다. 참가자들이 동등한 위치에서 합의를 향해 수렴한다. 모든 에이전트가 비슷한 편향을 공유하면 (같은 학습 분포를 공유하니까), 토론은 자신감 있게 틀린 답에 합의한다. 집단적 합의와 정확성은 다르다.

---

## 다른 질문

평가자에게 근본적으로 다른 질문을 할 수 있다:

*"이 솔루션이 실패하는 구체적인 입력을 찾아라."*

이건 open-ended evaluation이 아니다. 구체적인 탐색 과제다. 품질을 판단하거나 개선 사항을 제안하라는 게 아니다. 솔루션을 *깨트리라*는 것이다. 반례를 생성하라. 크래시, 오답, 제약 조건 위반을 유발하는 입력을 보여달라.

이것이 반증(falsification)이다. Karl Popper가 1934년에 주장한 것: 이론을 검증만으로 증명할 수는 없다. 확인하는 사례가 무한히 필요하기 때문이다. 하지만 하나의 반례로 이론을 반증할 수 있다. 과학은 가설을 확인함으로써 진보하지 않는다. 가설을 반증하는 데 실패함으로써 진보한다.

우리는 이 원리 위에 GCRI (Generalized Cognitive Refinement Iteration)를 만들었다. 에이전트에게 자기 작업을 되돌아보라고 하는 대신, 각 솔루션을 공격하는 것만이 임무인 전담 Red Team 에이전트를 배치했다:

```
PROCEDURE (Verification Agent):

1. LOGICAL ANALYSIS (코드 실행 전에 반드시 수행):
   - 코드를 주의 깊게 읽어라. 로직이 태스크 의도와 일치하는가?
   - 실행 성공이 논리적 정확성을 의미하지 않는다.
   - 항상 True를 반환하는 테스트는 무의미하다.

2. EXECUTE & VERIFY:
   - 코드 -> 실행. 크래시 = STRONG counter-example.
   - Generator가 작성한 테스트 통과는 정확성의 증거가 아니다.

3. ON FAILURE:
   - 가장 대표적인 실패를 counter_example로 선택
   - 가설을 직접 수정 (최소한의 편집)
```

검증 에이전트는 "이거 괜찮아?"라고 묻지 않는다. "이걸 깨트릴 수 있어?"라고 묻고, 깨트릴 수 있으면 깨트린다.

---

## 구별이 왜 중요한가

Self-reflection과 falsification은 멀리서 보면 비슷하다. 둘 다 생성 후 평가 단계가 있다. 둘 다 피드백으로 다음 시도를 개선한다. 하지만 세 가지 지점에서 결정적으로 갈린다.

**1. 인지 과제가 다르다.**

품질 평가는 판단의 영역이다. 반례 생성은 탐색 문제다. LLM은 판단보다 탐색에 훨씬 능하다. "이 함수가 잘못된 값을 반환하는 입력을 찾아라"고 하면 입력을 체계적으로 시도할 수 있다. "이 함수가 맞아?"라고 하면 모든 가능한 입력에 대해 동시에 추론해야 하는데, 이 지점에서 무너진다.

**2. 피드백이 언어적이 아니라 구조적이다.**

Reflexion은 "edge case를 고려했어야 했다" 같은 자연어 반성문을 저장한다. 모호하다. 도움이 될 수도 있고 아닐 수도 있다. 반례는 구체적이다: "입력 [3, -1, 0]에서 출력이 -1이어야 하는데 2가 나온다." 다음 iteration은 모호한 조언을 해석할 필요가 없다. 특정 실패 케이스를 처리하면 된다.

GCRI에서 실패한 반례는 **Active Constraints**로 변환된다. iteration을 넘어 영구적으로 유지되는 논리 규칙이다. 실패한 전략은 **Strategy Graveyard**에 들어가서 시스템이 같은 접근을 반복하는 것을 방지한다. 이것은 episodic memory가 아니다. 실패로부터의 구조화된 지식 추출이다.

**3. 평가자가 협력적이 아니라 적대적이다.**

Self-reflection에서 평가자는 돕려고 한다. 출력이 성공하길 바란다. 이것이 sycophancy 문제를 만든다. GCRI에서 검증 에이전트는 가설을 *격파*하려 한다. 인센티브 구조가 반전되어 있다: 검증 에이전트에게 성공이란 결함을 찾는 것이다. 네 논문이 통과하길 바라는 peer reviewer와 치명적 오류를 찾고 있는 reviewer의 차이다.

---

## 아키텍처

GCRI는 여러 가설을 병렬로, 각각 격리된 샌드박스에서 실행한다. 각 가설은 별개의 전략에서 생성되고, 내부 추론을 통해 정제된 후, Red Team 에이전트의 반증에 넘겨진다. 반증에서 살아남은 솔루션만 최종 Decision Maker에게 전달된다. 실패한 솔루션은 로그에 기록되고, 실패 패턴이 추출되어, 다음 iteration에 제약 조건으로 주입된다.

흐름:

1. **Strategy Generator**가 문제를 분석하고 N개의 서로 다른 접근법을 생성한다.
2. **Hypothesis Generator**가 각 전략을 격리된 샌드박스에서 구현한다 (브랜치당 하나).
3. **Reasoning Agent**가 검증 전에 각 가설을 정제한다.
4. **Verification Agent**가 구체적인 반례로 각 가설의 반증을 시도한다.
5. **Decision Maker**가 살아남은 가설을 선택하거나, 전부 거부하고 새 iteration을 시작한다.
6. **Memory Manager**가 실패를 영구적 Active Constraints로 변환한다.

이것은 collaborative refinement가 아니다. competitive survival이다. 가설이 부드러운 피드백을 통해 "개선"되는 게 아니다. 스트레스 테스트를 받고, 깨지면 죽는다. 살아남으면 심사를 받는다.

---

## 에코 챔버 문제

아키텍처 미학 이상의 이유로 이것이 중요하다.

LLM 에이전트에는 에코 챔버 문제가 있다. 같은 모델이 생성하고 평가하면, 자기 편향을 확인하는 경향이 있다. 같은 모델의 복사본 여럿이 토론하면, 공유된 편향 쪽으로 수렴한다. Huang et al.의 결과는 이상치가 아니다. 출력을 생성한 것과 같은 분포적 선행지식을 사용해서 자기 출력을 평가하도록 요청한 결과다.

반증은 에코 챔버를 깬다. 검증 과제가 비대칭이기 때문이다. 생성자는 요구사항 충족을 시도한다. 검증자는 요구사항 파괴를 시도한다. 서로 다른 최적화 목적이고, 같은 기반 모델조차 태스크 프레이밍이 "올바른 솔루션 만들기"에서 "이 코드를 크래시시키는 입력 하나 찾기"로 바뀌면 다르게 행동한다.

GCRI의 검증 프롬프트에 이 비대칭을 포착하는 문장이 있다:

> *"Generator가 작성한 테스트 통과는 정확성의 증거가 아니다. 출력이 원래 태스크 의도를 충족하는지 독립적으로 검증해야 한다."*

검증 에이전트는 명시적으로 생성자의 테스트 스위트를 신뢰하지 말라고 지시받는다. 같은 에이전트가 코드와 테스트와 리뷰를 전부 작성하는 self-reflection과 정반대다.

---

## 결과

이론으로 설계하고 잘 되기를 바란 것이 아니다. 실행했다.

중급 베이스 모델 (GPT-OSS-120B)을 사용한 GCRI 결과:

**HumanEval:** 71.0% → 95.1% (+24.1%)
**TheoremQA:** 56.8% → 72.9% (+16.1%)
**ARC-AGI-1:** 11.3% → 25.8% (+14.5%)

HumanEval 95.1%는 o1-preview (92.4%)와 Claude 3.5 Sonnet (92.0%)의 single agent 성능을 넘는다. TheoremQA 72.9%는 Claude 3.5 Sonnet (58.2%)을 넘는다. 모든 에이전트 역할에 같은 베이스 모델을 사용했으니, 성능 향상은 순수하게 아키텍처에서 왔다.

**BigCodeBench-Hard (Gemini-3-Flash):** 27.7% → 37.2% (+9.5%), Claude 3.5 Sonnet의 single agent 점수 35.8%를 넘는다.

숫자 자체보다 중요한 것은 그 함의다: 모델이 자기 자신에게 말하는 것과 적대자에게 스트레스 테스트를 받는 것 사이의 성능 격차가 크다.

---

## 문헌에서 빠져 있는 것

현재 LLM 피드백 프레임워크를 두 축으로 매핑할 수 있다: 누가 피드백을 주는가 (자기 vs. 외부), 어떤 피드백인가 (언어적 비평 vs. 구체적 반증).

```
                     언어적 비평              구체적 반증
                     ──────────────────      ──────────────
자기                  Reflexion,              (비어 있음)
                     Self-Refine

외부 (대칭적)         Multi-Agent Debate       (비어 있음)

외부 (적대적)         (비어 있음)               GCRI  ← 여기
```

우측 하단 칸이 문헌에서 거의 비어 있다. Constitutional AI (Anthropic)는 적대적 red-teaming을 사용하지만 학습 시점이지 추론 시점이 아니다. AIGS (AI-Generated Science)는 FalsificationAgent를 쓰지만 과학적 가설 검증 맥락이고, 범용 추론이 아니다. LATS는 가치 추정을 동반한 tree search를 쓰는데, 적대적 테스트보다 탐색에 가깝다.

GCRI는 적대적 평가와 구조화된 메모리의 교차점에 위치한다. 검증 에이전트가 반례를 생성한다. 메모리 시스템이 그 반례를 영구적 제약 조건으로 변환한다. 다중 브랜치 아키텍처가 가설의 다양성을 보장해서, 반증이 실제로 다른 접근법들에 걸쳐 선택 압력으로 작용한다.

---

## 실무적 시사점

에이전트 시스템을 만들면서 피드백 루프를 어떻게 추가할지 고민한다면, reflection과 falsification의 선택이 단순한 이론 논쟁이 아니다.

**Self-reflection이 적합한 경우:** 태스크가 주관적이거나 (글의 품질, 디자인 선택), 모델이 자기 출력을 프로그래밍적으로 검증할 방법이 없거나, 정확성보다 스타일을 최적화할 때.

**Falsification이 적합한 경우:** 태스크에 검증 가능한 정확성 기준이 있거나 (코드, 수학, 논리, 사실적 주장), 반례를 자동으로 생성하거나 테스트할 수 있거나, 출력이 edge case를 처리한다는 보장이 필요할 때.

엔지니어가 신경 쓰는 대부분의 태스크가 두 번째 범주에 해당한다. 코드는 실행되거나 안 되거나다. 수학 증명은 유효하거나 아니거나다. 논리적 주장은 따져보면 성립하거나 무너지거나다. 이런 태스크에 "이거 깨트릴 수 있어?"라고 묻는 것이 "이거 괜찮아 보여?"보다 엄밀하게 더 유용한 피드백을 만든다.

---

## 결론

Self-reflection 패러다임은 LLM이 자기 출력을 신뢰성 있게 판단할 수 있다는 가정 위에 세워져 있다. 실험적 증거는 그 반대를 말한다. 반증은 대안을 제시한다: 모델에게 자기 평가를 시키는 대신, 다른 일을 준다. 공격하라. 결함을 찾아라. 솔루션이 틀렸음을 증명하라.

증명하지 못하면, 비로소 올바른 답일 수 있다.

---

*GCRI는 오픈소스이며 [github.com/Dirac-Robot/GCRI](https://github.com/Dirac-Robot/GCRI)에서 사용 가능하다.*
